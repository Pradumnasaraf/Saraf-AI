## Saraf AI

**Saraf AI** is a chat assistant built with **Next.js**, using the OpenAI API framework to interface with the [Docker Model Runner](https://docs.docker.com/ai/model-runner). It runs entirely locally, ensuring your data remains private. Communication between the application and the LLM model is handled using the latest Docker Compose support for [running LLM models](https://docs.docker.com/ai/compose/models-and-compose/), enabling seamless and secure integration.

## Demo

https://github.com/user-attachments/assets/32fb49de-336b-406a-89ad-c1e9fc80fa60

## Prerequisites

- Docker and Docker Compose (version **2.38.0** or newer)

## Getting Started

1. Clone this repository.
2. Run `docker compose up`. This launches both the LLM model and the Next.js frontend. (Yes, it's that simple :))
3. Visit `http://localhost:3000` in your browser.

## Docker Compose Configuration

```yaml
services:
  saraf-ai:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - 3000:3000
    # This is the model that will be used by the app to connect to the local LLM.
    models: 
      - llm

models:
  llm:
    model: ai/smollm2
```

This is a standard Docker Compose setup with an additional `models` block that lets you define and run multiple AI models within the same file. You can swap out `ai/smollm2` for any other model available in the [DockerHub AI Catalog](https://hub.docker.com/catalogs/gen-ai).

> The `saraf-ai` service (Next.js app) requires `LLM_URL` and `LLM` environment variables to function. These are automatically generated by Docker and injected into the Next.js environment. The app then uses them to connect to the local LLM instance.

## License

This project is licensed under the Apache-2.0 license - see the [LICENSE](LICENSE) file for details.

## Security

If you discover a security vulnerability within this project, please check the [SECURITY](SECURITY.md) for more information.
